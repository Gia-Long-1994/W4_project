{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('test.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# full_text = pd.concat([train,test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    review=re.sub('[^a-zA-Z]',' ',review)\n",
    "    review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
    "    review=' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_review']=train['Phrase'].apply(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null rows\n",
    "- **After apply clean_review , null rows are arised ! This is maybe numbers were availaible before**\n",
    "- **This is because we subtitute them by *nothing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_row_train = train[train['clean_review'].values ==''].index.to_list()\n",
    "train.drop(null_row_train,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155901, 5)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean_review']=test['Phrase'].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_row_test = test[test['clean_review'].values ==''].index.to_list()\n",
    "test.drop(null_row_test,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66194, 4)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "full_text = pd.concat([train,test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222095, 5)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyst = []\n",
    "lyst_key = embeddings_dict.keys()\n",
    "for i in range(full_text.shape[0]) :\n",
    "    for word in full_text['clean_review'].values[i].split() : \n",
    "        if word not in lyst_key :\n",
    "            lyst.append(word)\n",
    "un_known_set = set(lyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(un_known_lyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_known_lyst = list(un_known_set)\n",
    "un_known_lyst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sau lần thứ 1 aborbing absorbing\n",
      "Sau lần thứ 2 accomodates accommodates\n",
      "Sau lần thứ 3 achero acero\n",
      "Sau lần thứ 4 achival archival\n",
      "Sau lần thứ 5 achronological chronological\n",
      "Sau lần thứ 6 actioners actioner\n",
      "Sau lần thứ 7 actorish actors\n",
      "Sau lần thứ 8 actorliness corniness\n",
      "Sau lần thứ 9 actory actor\n",
      "Sau lần thứ 10 addessi address\n",
      "Sau lần thứ 11 adorability durability\n",
      "Sau lần thứ 12 adventues adventures\n",
      "Sau lần thứ 13 affirmational affirmation\n",
      "Sau lần thứ 14 alientation alienation\n",
      "Sau lần thứ 15 allodi alloudi\n",
      "Sau lần thứ 16 amusedly amused\n",
      "Sau lần thứ 17 animaton animation\n",
      "Sau lần thứ 18 anteing anting\n",
      "Sau lần thứ 19 apallingly appallingly\n",
      "Sau lần thứ 20 artnering partnering\n",
      "Sau lần thứ 21 artsploitation exploitation\n",
      "Sau lần thứ 22 asiaphiles audiophiles\n",
      "Sau lần thứ 23 auteil auteuil\n",
      "Sau lần thứ 24 autocritique automatique\n",
      "Sau lần thứ 25 avventura aventura\n",
      "Sau lần thứ 26 baaaaaaaaad bananarama\n",
      "Sau lần thứ 27 bazadona maradona\n",
      "Sau lần thứ 28 bergmanesque romanesque\n",
      "Sau lần thứ 29 beseechingly beseeching\n",
      "Sau lần thứ 30 bibbidy bibby\n",
      "Sau lần thứ 31 bierbichler beichler\n",
      "Sau lần thứ 32 birot bigot\n",
      "Sau lần thứ 33 bizzarre bizarre\n",
      "Sau lần thứ 34 bjorkness bjerknes\n",
      "Sau lần thứ 35 blighter lighter\n",
      "Sau lần thứ 36 blutarsky lubarsky\n",
      "Sau lần thứ 37 bobbidi bobbili\n",
      "Sau lần thứ 38 bohos boos\n",
      "Sau lần thứ 39 bondish bodish\n",
      "Sau lần thứ 40 bornin bonin\n",
      "Sau lần thứ 41 bottomlessly bottomless\n",
      "Sau lần thứ 42 breakingly breaking\n",
      "Sau lần thứ 43 breheny bremen\n",
      "Sau lần thứ 44 bruckheimeresque bruckheimer\n",
      "Sau lần thứ 45 burningly burning\n",
      "Sau lần thứ 46 bustingly busting\n",
      "Sau lần thứ 47 butterfingered butterfinger\n",
      "Sau lần thứ 48 cadness madness\n",
      "Sau lần thứ 49 cannier canner\n",
      "Sau lần thứ 50 captivatingly captivating\n",
      "Sau lần thứ 51 chabrolian chabrol\n",
      "Sau lần thứ 52 chardonne charonne\n",
      "Sau lần thứ 53 cheatfully cheerfully\n",
      "Sau lần thứ 54 chopsocky chomsky\n",
      "Sau lần thứ 55 choquart hocquart\n",
      "Sau lần thứ 56 cineasts cineastes\n",
      "Sau lần thứ 57 cinemantic cinematic\n",
      "Sau lần thứ 58 cipherlike fingerlike\n",
      "Sau lần thứ 59 cirulnick sirulnick\n",
      "Sau lần thứ 60 claustrophic claustrophobic\n",
      "Sau lần thứ 61 clericks clerics\n",
      "Sau lần thứ 62 cletis cletus\n",
      "Sau lần thứ 63 clutchy clutch\n",
      "Sau lần thứ 64 codswallop cadwallon\n",
      "Sau lần thứ 65 collosum callosum\n",
      "Sau lần thứ 66 completist completists\n",
      "Sau lần thứ 67 condundrum conundrum\n",
      "Sau lần thứ 68 contructed constructed\n",
      "Sau lần thứ 69 copmovieland movieland\n",
      "Sau lần thứ 70 copyof copy\n",
      "Sau lần thứ 71 coriat coria\n",
      "Sau lần thứ 72 corniest contest\n",
      "Sau lần thứ 73 corruscating coruscating\n",
      "Sau lần thứ 74 costumey costume\n",
      "Sau lần thứ 75 courageousness outrageousness\n",
      "Sau lần thứ 76 crappola coppola\n",
      "Sau lần thứ 77 crapulence corpulence\n",
      "Sau lần thứ 78 crowdpleaser crowd-pleasing\n",
      "Sau lần thứ 79 crummles crumbles\n",
      "Sau lần thứ 80 datedness lateness\n",
      "Sau lần thứ 81 dateflick dateline\n",
      "Sau lần thứ 82 deadeningly deafeningly\n",
      "Sau lần thứ 83 debrauwer drawer\n",
      "Sau lần thứ 84 debuter debuted\n",
      "Sau lần thứ 85 decasia decani\n",
      "Sau lần thứ 86 defeatingly defeating\n",
      "Sau lần thứ 87 delibrately deliberately\n",
      "Sau lần thứ 88 democracie democracies\n",
      "Sau lần thứ 89 denlopp dunlop\n",
      "Sau lần thứ 90 derivativeness derivatives\n",
      "Sau lần thứ 91 destinees destinies\n",
      "Sau lần thứ 92 deutchland deutschland\n",
      "Sau lần thứ 93 diciness iciness\n",
      "Sau lần thứ 94 disposible disposable\n",
      "Sau lần thứ 95 divertingly diverting\n",
      "Sau lần thứ 96 djeinaba djellaba\n",
      "Sau lần thứ 97 dognini fognini\n",
      "Sau lần thứ 98 dogwalker walker\n",
      "Sau lần thứ 99 dooper cooper\n",
      "Sau lần thứ 100 dorkier dornier\n",
      "Sau lần thứ 101 doshas dosha\n",
      "Sau lần thứ 102 dreadfulness fearfulness\n",
      "Sau lần thứ 103 drek drew\n",
      "Sau lần thứ 104 dridi didi\n",
      "Sau lần thứ 105 drippiness drippings\n",
      "Sau lần thứ 106 dudsville dansville\n",
      "Sau lần thứ 107 dullingly willingly\n",
      "Sau lần thứ 108 dumbfoundingly dumbfounding\n",
      "Sau lần thứ 109 dungpile dogpile\n",
      "Sau lần thứ 110 dysfunctionally dysfunctional\n",
      "Sau lần thứ 111 eckstraordinarily extraordinarily\n",
      "Sau lần thứ 112 efteriades elefteriades\n",
      "Sau lần thứ 113 egocentricities eccentricities\n",
      "Sau lần thứ 114 elegiacally logically\n",
      "Sau lần thứ 115 emptily empty\n",
      "Sau lần thứ 116 encumbers encumber\n",
      "Sau lần thứ 117 engross gross\n",
      "Sau lần thứ 118 enrapturing capturing\n",
      "Sau lần thứ 119 equlibrium equilibrium\n",
      "Sau lần thứ 120 eroti erotic\n",
      "Sau lần thứ 121 espite despite\n",
      "Sau lần thứ 122 everlyn evelyn\n",
      "Sau lần thứ 123 excrescence excellence\n",
      "Sau lần thứ 124 exhilarate exhilarated\n",
      "Sau lần thứ 125 exporing exporting\n",
      "Sau lần thứ 126 fantasti fantastic\n",
      "Sau lần thứ 127 feardotcom fedotov\n",
      "Sau lần thứ 128 felinni feline\n",
      "Sau lần thứ 129 fillm film\n",
      "Sau lần thứ 130 fizzability liability\n",
      "Sau lần thứ 131 flakeball baseball\n",
      "Sau lần thứ 132 flatula flatly\n",
      "Sau lần thứ 133 flck flock\n",
      "Sau lần thứ 134 flibbertigibbet liberties\n",
      "Sau lần thứ 135 fluxing flexing\n",
      "Sau lần thứ 136 forgettably forgettable\n",
      "Sau lần thứ 137 formuliac formula\n",
      "Sau lần thứ 138 fuddled huddled\n",
      "Sau lần thứ 139 fuhgeddaboutit whereabouts\n",
      "Sau lần thứ 140 fustily lustily\n",
      "Sau lần thứ 141 gabbiest cabbies\n",
      "Sau lần thứ 142 gantzes ganzes\n",
      "Sau lần thứ 143 gasm gas\n",
      "Sau lần thứ 144 gerbosi verbose\n",
      "Sau lần thứ 145 glizty guilty\n",
      "Sau lần thứ 146 gooeyness goodness\n",
      "Sau lần thứ 147 goombah gombak\n",
      "Sau lần thứ 148 goosebump goosebumps\n",
      "Sau lần thứ 149 gorefests gorefest\n",
      "Sau lần thứ 150 greasiest greatest\n",
      "Sau lần thứ 151 guessable unstable\n",
      "Sau lần thứ 152 gunfest sunfest\n",
      "Sau lần thứ 153 gutterball butterball\n",
      "Sau lần thứ 154 gymkata gymkhana\n",
      "Sau lần thứ 155 halfwit halfway\n",
      "Sau lần thứ 156 hamfisted hamister\n",
      "Sau lần thứ 157 hammily hammill\n",
      "Sau lần thứ 158 haphazardness haphazard\n",
      "Sau lần thứ 159 hardass harass\n",
      "Sau lần thứ 160 hastier nastier\n",
      "Sau lần thứ 161 headbangingly headbanging\n",
      "Sau lần thứ 162 heartwarmingly heartwarming\n",
      "Sau lần thứ 163 hellstenius helenius\n",
      "Sau lần thứ 164 heremakono hermano\n",
      "Sau lần thứ 165 higuchinsky luchinsky\n",
      "Sau lần thứ 166 hirosue hirose\n",
      "Sau lần thứ 167 histo hristo\n",
      "Sau lần thứ 168 hitchcockianism hitchcockian\n",
      "Sau lần thứ 169 hjelje hjelle\n",
      "Sau lần thứ 170 hobnail hotmail\n",
      "Sau lần thứ 171 holofcenter holofcener\n",
      "Sau lần thứ 172 hotdogging dogging\n",
      "Sau lần thứ 173 hotsies hotties\n",
      "Sau lần thứ 174 hubac huac\n",
      "Sau lần thứ 175 humbuggery hamburger\n",
      "Sau lần thứ 176 idemoto ikemoto\n",
      "Sau lần thứ 177 idoosyncratic idiosyncratic\n",
      "Sau lần thứ 178 ihops shops\n",
      "Sau lần thứ 179 imaxy imax\n",
      "Sau lần thứ 180 imponderably imponderable\n",
      "Sau lần thứ 181 inconsequentiality inconsequential\n",
      "Sau lần thứ 182 indieflick indie-rock\n",
      "Sau lần thứ 183 inducingly inducing\n",
      "Sau lần thứ 184 inhospitability indomitability\n",
      "Sau lần thứ 185 intacto intact\n",
      "Sau lần thứ 186 interspliced interspersed\n",
      "Sau lần thứ 187 involvingly involving\n",
      "Sau lần thứ 188 italicizes italicized\n",
      "Sau lần thứ 189 jaglomized randomized\n",
      "Sau lần thứ 190 janklowicz janowicz\n",
      "Sau lần thứ 191 japanimator animator\n",
      "Sau lần thứ 192 jived lived\n",
      "Sau lần thứ 193 joylessly joyless\n",
      "Sau lần thứ 194 juiceless priceless\n",
      "Sau lần thứ 195 kahlories calories\n",
      "Sau lần thứ 196 kalesniko kolesnikov\n",
      "Sau lần thứ 197 kalvert calvert\n",
      "Sau lần thứ 198 kaputschnik kaushik\n",
      "Sau lần thứ 199 kazmierski kaminski\n",
      "Sau lần thứ 200 kibbitzes kibitzers\n",
      "Sau lần thứ 201 kickass kick-ass\n",
      "Sau lần thứ 202 kidlets killers\n",
      "Sau lần thứ 203 komediant comedian\n",
      "Sau lần thứ 204 kosashvili kobiashvili\n",
      "Sau lần thứ 205 koshashvili kobiashvili\n",
      "Sau lần thứ 206 laboriousness luxuriousness\n",
      "Sau lần thứ 207 landbound hardbound\n",
      "Sau lần thứ 208 lapdance laplace\n",
      "Sau lần thứ 209 laugther laugher\n",
      "Sau lần thứ 210 leatherbound leather-bound\n",
      "Sau lần thứ 211 leplouff leadoff\n",
      "Sau lần thứ 212 likableness likeness\n",
      "Sau lần thứ 213 luncher launcher\n",
      "Sau lần thứ 214 luvvies levies\n",
      "Sau lần thứ 215 makmalbaf makhmalbaf\n",
      "Sau lần thứ 216 manipulativeness manipulatives\n",
      "Sau lần thứ 217 marcken macken\n",
      "Sau lần thứ 218 margolo margalo\n",
      "Sau lần thứ 219 marveilleux marseille\n",
      "Sau lần thứ 220 masterpeice masterpiece\n",
      "Sau lần thứ 221 materalism materialism\n",
      "Sau lần thứ 222 mateys mates\n",
      "Sau lần thứ 223 mcklusky mcclusky\n",
      "Sau lần thứ 224 meaningness meaningless\n",
      "Sau lần thứ 225 meanspirited mean-spirited\n",
      "Sau lần thứ 226 mibii miami\n",
      "Sau lần thứ 227 minac minas\n",
      "Sau lần thứ 228 miscasts miscast\n",
      "Sau lần thứ 229 monkeyfun monkey\n",
      "Sau lần thứ 230 monsterous monstrous\n",
      "Sau lần thứ 231 montia monti\n",
      "Sau lần thứ 232 montied montiel\n",
      "Sau lần thứ 233 mouglalis douglas\n",
      "Sau lần thứ 234 mullinski mullins\n",
      "Sau lần thứ 235 musclefest muscles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sau lần thứ 236 narcotized narcotics\n",
      "Sau lần thứ 237 narcotizing harmonizing\n",
      "Sau lần thứ 238 naturedness naturalness\n",
      "Sau lần thứ 239 nebrida nebria\n",
      "Sau lần thứ 240 niblet nibley\n",
      "Sau lần thứ 241 nohe note\n",
      "Sau lần thứ 242 nolden golden\n",
      "Sau lần thứ 243 nonchallenging unchallenging\n",
      "Sau lần thứ 244 nonethnic non-ethnic\n",
      "Sau lần thứ 245 nonjudgmentally nonjudgmental\n",
      "Sau lần thứ 246 nrelentingly unrelentingly\n",
      "Sau lần thứ 247 nuld nld\n",
      "Sau lần thứ 248 nutjob nutso\n",
      "Sau lần thứ 249 nuttgens mutagens\n",
      "Sau lần thứ 250 nymphette nymphet\n",
      "Sau lần thứ 251 obviation aviation\n",
      "Sau lần thứ 252 ohlinger olinger\n",
      "Sau lần thứ 253 ooky kooky\n",
      "Sau lần thứ 254 oprahfication certification\n",
      "Sau lần thứ 255 ourside outside\n",
      "Sau lần thứ 256 outgag outrage\n",
      "Sau lần thứ 257 overemphatic overemphasis\n",
      "Sau lần thứ 258 overmanipulative manipulative\n",
      "Sau lần thứ 259 overplotted overlooked\n",
      "Sau lần thứ 260 overstylized oversized\n",
      "Sau lần thứ 261 ozpetek petek\n",
      "Sau lần thứ 262 pantomimesque pantomimes\n",
      "Sau lần thứ 263 pasach pasch\n",
      "Sau lần thứ 264 penotti menotti\n",
      "Sau lần thứ 265 perfervid performed\n",
      "Sau lần thứ 266 petin putin\n",
      "Sau lần thứ 267 phonce phone\n",
      "Sau lần thứ 268 pistoled pistole\n",
      "Sau lần thứ 269 plaintiveness positiveness\n",
      "Sau lần thứ 270 pokepie poeple\n",
      "Sau lần thứ 271 pollyana pollyanna\n",
      "Sau lần thứ 272 powaqqatsi powiats\n",
      "Sau lần thứ 273 powerment empowerment\n",
      "Sau lần thứ 274 prechewed preceded\n",
      "Sau lần thứ 275 precollegiate collegiate\n",
      "Sau lần thứ 276 prefeminist pro-feminist\n",
      "Sau lần thứ 277 pretention prevention\n",
      "Sau lần thứ 278 prewarned prepared\n",
      "Sau lần thứ 279 provocatuers provocateurs\n",
      "Sau lần thứ 280 psychodramatics psychodramas\n",
      "Sau lần thứ 281 pulpiness puffiness\n",
      "Sau lần thứ 282 puportedly purportedly\n",
      "Sau lần thứ 283 puttingly putting\n",
      "Sau lần thứ 284 qatsi qaisi\n",
      "Sau lần thứ 285 qutting putting\n",
      "Sau lần thứ 286 recoing redoing\n",
      "Sau lần thứ 287 reconceptualize conceptualize\n",
      "Sau lần thứ 288 reeboir reebok\n",
      "Sau lần thứ 289 reeses reeves\n",
      "Sau lần thứ 290 repellantly repellant\n",
      "Sau lần thứ 291 repulsively repulsive\n",
      "Sau lần thứ 292 retitle retitled\n",
      "Sau lần thứ 293 revigorates invigorates\n",
      "Sau lần thứ 294 ricture picture\n",
      "Sau lần thứ 295 roisterous boisterous\n",
      "Sau lần thứ 296 romething something\n",
      "Sau lần thứ 297 romoli romola\n",
      "Sau lần thứ 298 runteldat intelsat\n",
      "Sau lần thứ 299 russos russo\n",
      "Sau lần thứ 300 ryanovich ivanovich\n",
      "Sau lần thứ 301 sailboaters sailboats\n",
      "Sau lần thứ 302 salaciously salacious\n",
      "Sau lần thứ 303 sanctimoniousness sanctimonious\n",
      "Sau lần thứ 304 sandlerian adlerian\n",
      "Sau lần thứ 305 sappier happier\n",
      "Sau lần thứ 306 schneidermeister reidemeister\n",
      "Sau lần thứ 307 schticky schtick\n",
      "Sau lần thứ 308 scoob scoop\n",
      "Sau lần thứ 309 scripters scripter\n",
      "Sau lần thứ 310 scuzbag scumbag\n",
      "Sau lần thứ 311 seldahl meldahl\n",
      "Sau lần thứ 312 seldhal stendhal\n",
      "Sau lần thứ 313 semimusical musical\n",
      "Sau lần thứ 314 sermonize sermonized\n",
      "Sau lần thứ 315 servicable serviceable\n",
      "Sau lần thứ 316 shagster shatter\n",
      "Sau lần thứ 317 shakesperean shakespearean\n",
      "Sau lần thứ 318 shapable capable\n",
      "Sau lần thứ 319 shapelessly shamelessly\n",
      "Sau lần thứ 320 shayamalan shyamalan\n",
      "Sau lần thứ 321 sheerly sheely\n",
      "Sau lần thứ 322 shimmeringly shimmering\n",
      "Sau lần thứ 323 shlockmeister hofmeister\n",
      "Sau lần thứ 324 shmear smear\n",
      "Sau lần thứ 325 shoplifts shoplift\n",
      "Sau lần thứ 326 shrieky shrieks\n",
      "Sau lần thứ 327 silbersteins silberstein\n",
      "Sau lần thứ 328 sillified villified\n",
      "Sau lần thứ 329 sitcomishly sitcoms\n",
      "Sau lần thứ 330 siuation situation\n",
      "Sau lần thứ 331 skeeved sleeved\n",
      "Sau lần thứ 332 skippable skiable\n",
      "Sau lần thứ 333 slappingly slapping\n",
      "Sau lần thứ 334 slowtime showtime\n",
      "Sau lần thứ 335 slummer summer\n",
      "Sau lần thứ 336 slummy plummy\n",
      "Sau lần thứ 337 smashups mashups\n",
      "Sau lần thứ 338 snazziness snazziest\n",
      "Sau lần thứ 339 soaringly sparingly\n",
      "Sau lần thứ 340 softheaded hotheaded\n",
      "Sau lần thứ 341 sogginess grogginess\n",
      "Sau lần thứ 342 solondzian solondz\n",
      "Sau lần thứ 343 sparklingly sparkling\n",
      "Sau lần thứ 344 spittingly spitting\n",
      "Sau lần thứ 345 splatterfests splattered\n",
      "Sau lần thứ 346 splittingly splitting\n",
      "Sau lần thứ 347 spookies spookier\n",
      "Sau lần thứ 348 squaddie square\n",
      "Sau lần thứ 349 steinis stennis\n",
      "Sau lần thứ 350 sterotypes stereotypes\n",
      "Sau lần thứ 351 stevenon stevenson\n",
      "Sau lần thứ 352 stoppingly stopping\n",
      "Sau lần thứ 353 stortelling storytelling\n",
      "Sau lần thứ 354 strafings strafing\n",
      "Sau lần thứ 355 strainingly straining\n",
      "Sau lần thứ 356 stuffiest stuffit\n",
      "Sau lần thứ 357 stultifyingly stultifying\n",
      "Sau lần thứ 358 stumblings stumbling\n",
      "Sau lần thứ 359 stuporously humorously\n",
      "Sau lần thứ 360 substitutable substitute\n",
      "Sau lần thứ 361 superficiale superficial\n",
      "Sau lần thứ 362 superheroics superheroic\n",
      "Sau lần thứ 363 superlarge supercharge\n",
      "Sau lần thứ 364 surehanded barehanded\n",
      "Sau lần thứ 365 surfacey surface\n",
      "Sau lần thứ 366 suspenser suspense\n",
      "Sau lần thứ 367 swordfights swordfight\n",
      "Sau lần thứ 368 sychowski wachowski\n",
      "Sau lần thứ 369 sytle lytle\n",
      "Sau lần thứ 370 talkiness tackiness\n",
      "Sau lần thứ 371 tardier hardier\n",
      "Sau lần thứ 372 tatter matter\n",
      "Sau lần thứ 373 teendom tendon\n",
      "Sau lần thứ 374 telanovela telenovela\n",
      "Sau lần thứ 375 thekids thesis\n",
      "Sau lần thứ 376 thesps thesis\n",
      "Sau lần thứ 377 thons tons\n",
      "Sau lần thứ 378 throe three\n",
      "Sau lần thứ 379 thumpingly thumping\n",
      "Sau lần thứ 380 timewaster tidewater\n",
      "Sau lần thứ 381 toolbags toolbars\n",
      "Sau lần thứ 382 transfigures transfigured\n",
      "Sau lần thứ 383 travil travel\n",
      "Sau lần thứ 384 truckzilla drucilla\n",
      "Sau lần thứ 385 truncheoning truncheons\n",
      "Sau lần thứ 386 tryingly trying\n",
      "Sau lần thứ 387 uberviolence powerviolence\n",
      "Sau lần thứ 388 unamusing amusing\n",
      "Sau lần thứ 389 uncharismatically charismatically\n",
      "Sau lần thứ 390 uncinematic cinematic\n",
      "Sau lần thứ 391 unclassifiably unclassifiable\n",
      "Sau lần thứ 392 unconned unmanned\n",
      "Sau lần thứ 393 underachieves underachiever\n",
      "Sau lần thứ 394 underconfident overconfident\n",
      "Sau lần thứ 395 underdramatized overdramatized\n",
      "Sau lần thứ 396 underrehearsed unrehearsed\n",
      "Sau lần thứ 397 underventilated underestimated\n",
      "Sau lần thứ 398 undeterminable indeterminable\n",
      "Sau lần thứ 399 unembarrassing embarrassing\n",
      "Sau lần thứ 400 unemotive emotive\n",
      "Sau lần thứ 401 unencouraging encouraging\n",
      "Sau lần thứ 402 unentertaining entertaining\n",
      "Sau lần thứ 403 unfakable unshakable\n",
      "Sau lần thứ 404 unforgivingly unforgiving\n",
      "Sau lần thứ 405 unfussily unfussy\n",
      "Sau lần thứ 406 unhibited inhibited\n",
      "Sau lần thứ 407 unhidden unbidden\n",
      "Sau lần thứ 408 unimpressively impressively\n",
      "Sau lần thứ 409 uninventive inventive\n",
      "Sau lần thứ 410 unlaughable laughable\n",
      "Sau lần thứ 411 unplundered plundered\n",
      "Sau lần thứ 412 unrecommendable recommendable\n",
      "Sau lần thứ 413 unreligious religious\n",
      "Sau lần thứ 414 unsalvageability unavailability\n",
      "Sau lần thứ 415 unshapely shapely\n",
      "Sau lần thứ 416 unslick unstick\n",
      "Sau lần thứ 417 unsuspenseful suspenseful\n",
      "Sau lần thứ 418 untidily untimely\n",
      "Sau lần thứ 419 untugged unplugged\n",
      "Sau lần thứ 420 uplifter uplifted\n",
      "Sau lần thứ 421 ventually eventually\n",
      "Sau lần thứ 422 verete verite\n",
      "Sau lần thứ 423 videologue ideologue\n",
      "Sau lần thứ 424 vidgame midgame\n",
      "Sau lần thứ 425 villians willians\n",
      "Sau lần thứ 426 volletta valletta\n",
      "Sau lần thứ 427 vulakoro lavoro\n",
      "Sau lần thứ 428 wankery wanker\n",
      "Sau lần thứ 429 watstein ramstein\n",
      "Sau lần thứ 430 waydowntown downtown\n",
      "Sau lần thứ 431 wewannour serandour\n",
      "Sau lần thứ 432 whimsicality whimsically\n",
      "Sau lần thứ 433 wifty fifty\n",
      "Sau lần thứ 434 windtalker windtalkers\n",
      "Sau lần thứ 435 wisegirls wiseguys\n",
      "Sau lần thứ 436 witlessness listlessness\n",
      "Sau lần thứ 437 wollter wolter\n",
      "Sau lần thứ 438 wonderous wondrous\n",
      "Sau lần thứ 439 zillionth millionth\n",
      "Sau lần thứ 440 zishe she\n",
      "Sau lần thứ 441 zzzzzzzzz pizzazz\n"
     ]
    }
   ],
   "source": [
    "modify_lyst = []\n",
    "i = 1\n",
    "for word_1 in un_known_lyst : \n",
    "    dict_ = {}\n",
    "    for word_2 in embeddings_dict.keys() :\n",
    "        dict_[word_2] = nltk.edit_distance(word_1,word_2)\n",
    "    modify_lyst.append(min(dict_,key=dict_.get))\n",
    "    print(f'Sau lần thứ {i}',word_1,modify_lyst[-1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save *un_known_lyst*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"un_known.txt\", \"wb\") as uk: \n",
    "    pickle.dump(un_known_lyst, uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save *modify_lyst*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"well_known.txt\", \"wb\") as wk: \n",
    "    pickle.dump(modify_lyst, wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build array X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize array X by the very first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyst_initialize = []\n",
    "for word in full_text['clean_review'].values[0].split() :\n",
    "        lyst_initialize.append(embeddings_dict[word])\n",
    "a_initialize = np.array(lyst_initialize)\n",
    "b_initialize = np.sum(a_initialize,axis=0)\n",
    "max_ = np.max(b_initialize)\n",
    "min_ = np.min(b_initialize)\n",
    "X = (b_initialize-min_)/(max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load *un_known_lyst*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"un_known.txt\", \"rb\") as uk :\n",
    "    un_known_list = pickle.load(uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load *modify_lyst*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"well_known.txt\", \"rb\") as wk:\n",
    "    modify_list = pickle.load(wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10001 times vstack\n",
      "After 20001 times vstack\n",
      "After 30001 times vstack\n",
      "After 40001 times vstack\n",
      "After 50001 times vstack\n",
      "After 60001 times vstack\n",
      "After 70001 times vstack\n",
      "After 80001 times vstack\n",
      "After 90001 times vstack\n",
      "After 100001 times vstack\n",
      "After 110001 times vstack\n",
      "After 120001 times vstack\n",
      "After 130001 times vstack\n",
      "After 140001 times vstack\n",
      "After 150001 times vstack\n",
      "After 160001 times vstack\n",
      "After 170001 times vstack\n",
      "After 180001 times vstack\n",
      "After 190001 times vstack\n",
      "After 200001 times vstack\n",
      "After 210001 times vstack\n",
      "After 220001 times vstack\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,full_text.shape[0]) :\n",
    "    li = []\n",
    "    for word in full_text['clean_review'].values[i].split() :\n",
    "        if word in un_known_list :\n",
    "            index_ = un_known_list.index(word)\n",
    "            modify_word = modify_list[index_]\n",
    "            li.append(embeddings_dict[modify_word])\n",
    "        else :\n",
    "            li.append(embeddings_dict[word])\n",
    "    a = np.array(li)\n",
    "    b = np.sum(a,axis=0)\n",
    "    max_ = np.max(b)\n",
    "    min_ = np.min(b)\n",
    "    b_ = (b - min_)/(max_-min_)\n",
    "    X = np.vstack((X,b_))\n",
    "    if i%10000 == 0 :\n",
    "        print(f'After {X.shape[0]} times vstack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save array X as *file.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X.txt', X , fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply one - hot coding Model from Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:train.shape[0],:]\n",
    "y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ASUS\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 55.15%, std 0.14.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, X_train , y_train , scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ovr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 2)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66194, 4)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Sentiment'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'Sentiment':'sentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(sub,test,how='left',on='PhraseId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['sentiment'].fillna(2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId         0\n",
       "Sentiment        0\n",
       "SentenceId      98\n",
       "Phrase          98\n",
       "clean_review    98\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = result[['PhraseId','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('Dinh_Nguyen_submit_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 2)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
